{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "IMAGE CLASSIFICATION WITH TENSOR FLOW AND OPEN CV... Custom model is used ;) \n",
    "\n",
    "\n",
    "0 left,\n",
    "1 right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "from PIL import Image\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import socket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SOCKET INIT\n",
    "\n",
    "# host = '192.168.43.3' #Raspi address\n",
    "# port = 3032 #communication port\n",
    "# sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "# sock.connect((host, port))\n",
    "\n",
    "def close_socket():\n",
    "    sock.close()\n",
    "    \n",
    "def send_str_socket(string):\n",
    "    command = string\n",
    "    #sock.send(command.encode('utf-8'))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable scientific notation for clarity\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the model\n",
    "model = tensorflow.keras.models.load_model('keras_model.h5', compile=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.ndarray(shape=(1, 224, 224, 3), dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FUNCTION TO OVERLAY TEXT ON Cv2 Video Capture\n",
    "\n",
    "def print_cv2(text, x, y, b, g, r):\n",
    "    cv2.putText(image_feed, text, (x, y), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (b, g, r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TRIGGERS FOR SOCKET\n",
    "\n",
    "class0 = False\n",
    "class1 = False\n",
    "class2 = False\n",
    "class3 = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ALL THE CONDITIONS BASED ON THE PREDICTIONS\n",
    "#[0, X] where X is the number of classes in the model\n",
    "\n",
    "\n",
    "def prediction_conditions():\n",
    "    \n",
    "    global class0, class1, class2, class3\n",
    "    \n",
    "    #BOTTOM BOX FOR PREDICTION TEXT\n",
    "    cv2.rectangle(image_feed, (5, 440), (430, 470), (0, 0, 0), -1)\n",
    "    \n",
    "    #TOP BOX FOR OUTCOME\n",
    "    cv2.rectangle(image_feed, (5, 10), (80, 40), (255, 255, 255), -1)\n",
    "    \n",
    "    print_cv2(str(prediction[0]), 10, 460, 255, 255, 255)\n",
    "    \n",
    "    if ((prediction[0, 0] > prediction[0, 1]) and (prediction[0, 0] > prediction[0, 2]) and (prediction[0, 0] > prediction[0, 3])):\n",
    "        cv2.rectangle (image_feed, (0, 0), (640, 480), (0, 255, 0), 5) #FIRST CONDITION, green color\n",
    "        print_cv2(\"Up\", 10, 30, 0, 255, 0)\n",
    "        if class0 == False:\n",
    "            send_str_socket('up')\n",
    "            class0 = True\n",
    "            class1 = False\n",
    "            class2 = False\n",
    "            class3 = False\n",
    "     \n",
    "    elif ((prediction[0, 1] > prediction[0, 0]) and (prediction[0, 1] > prediction[0, 2]) and (prediction[0, 1] > prediction[0, 3])):\n",
    "        cv2.rectangle (image_feed, (0, 0), (640, 480), (255, 0, 0), 5) #SECOND CONDITION, blue color\n",
    "        print_cv2(\"Down\", 10, 30, 255, 0, 0)\n",
    "        if class1 == False:\n",
    "            send_str_socket('down')\n",
    "            class0 = False\n",
    "            class1 = True\n",
    "            class2 = False\n",
    "            class3 = False\n",
    "        \n",
    "    elif ((prediction[0, 2] > prediction[0, 0]) and (prediction[0, 2] > prediction[0, 1]) and (prediction[0, 2] > prediction[0, 3])):\n",
    "        cv2.rectangle (image_feed, (0, 0), (640, 480), (0, 0, 255), 5) #THIRD CONDITION, red color \n",
    "        print_cv2(\"Left\", 10, 30, 0, 0, 255)\n",
    "        if class2 == False:\n",
    "            send_str_socket('left')\n",
    "            class0 = False\n",
    "            class1 = False\n",
    "            class2 = True\n",
    "            class3 = False\n",
    "        \n",
    "    elif ((prediction[0, 3] > prediction[0, 0]) and (prediction[0, 3] > prediction[0, 1]) and (prediction[0, 3] > prediction[0, 2])):\n",
    "        cv2.rectangle (image_feed, (0, 0), (640, 480), (0, 255, 255), 5) # FOURTH COMDITION, yellow color\n",
    "        print_cv2(\"Right\", 10, 30, 0, 255, 255)\n",
    "        if class3 == False:\n",
    "            send_str_socket('right')\n",
    "            class0 = False\n",
    "            class1 = False\n",
    "            class2 = False\n",
    "            class3 = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#listen onto the droid cam video feed.\n",
    "#Use cv2.VideoCapture(0) to use wired usb camera. Where 0 is the index of the camera\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "#Main loop\n",
    "while True:\n",
    "    \n",
    "    ret, image_feed = cap.read()\n",
    "    \n",
    "    image_feed = cv2.flip(image_feed, 1) #Flip the video horizontally\n",
    "    \n",
    "    M = cv2.getRotationMatrix2D( (320, 240), 270, 1.42)\n",
    "    rotated_image = cv2.warpAffine( image_feed, M, (480, 640))\n",
    "    \n",
    "    #cv2.imshow(\"Output 0, raw\", image_feed)\n",
    "    \n",
    "    #tik = time.time()\n",
    "    \n",
    "    cv2.imwrite('pics\\output.jpg', rotated_image)\n",
    "    \n",
    "    image = Image.open('pics\\output.jpg')\n",
    "    \n",
    "    #tok = time.time()\n",
    "    \n",
    "    image = image.resize((224, 224))\n",
    "    image_array = np.asarray(image)\n",
    "    normalized_image_array = image_array / 255.0\n",
    "    data[0] = normalized_image_array\n",
    "    prediction = model.predict(data)\n",
    "    \n",
    "    #tuk = time.time()\n",
    "    \n",
    "    #PRINT THE TIME DIFFERENCE BETWEEN FUNCTIONS\n",
    "    '''print(\"Time to save and read image: \" + str(tok - tik))\n",
    "    print(\"Time to classify image: \" + str(tuk - tok))\n",
    "    print(\"Total time wasted: \" + str((tok - tik) + (tuk - tok)))'''\n",
    "    \n",
    "    prediction_conditions()\n",
    "    \n",
    "    #cv2.imshow('OUTPUT 0', flip_feed)\n",
    "    cv2.imshow('output 0, raw', rotated_image)\n",
    "    \n",
    "    if cv2.waitKey(1) & 0xff == ord('q'):\n",
    "        break\n",
    "        \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "close_socket()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
